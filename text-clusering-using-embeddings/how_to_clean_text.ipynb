{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa416e-031d-4e4b-9253-525128b9b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4ee346-ac26-48f7-b778-d7fe9f5b4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e96143f-0f00-4c96-857e-02b3a16eb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html):\n",
    "\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for data in soup(['style', 'script', 'code', 'a']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "\n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30149f89-38d4-4b7e-ac25-bfa4c0370a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_string = \"\"\"\n",
    "<p>\n",
    "  <a\n",
    "    href=\"https://forge.autodesk.com/#step-6-download-the-item\"\n",
    "    rel=\"nofollow noreferrer\"\n",
    "    >https://forge.autodesk.com/en//#step-6-download-the-item</a\n",
    "  >\n",
    "</p>\n",
    "\\n\\n\n",
    "<p>\n",
    "  I have followed the tutorial and have successfully obtained the contents of\n",
    "  the file, but where is the file being downloaded. In addition, how do I\n",
    "  specify the location of where I want to download the file?\n",
    "</p>\n",
    "\\n\\n\n",
    "<p>\n",
    "  Result on Postman\\n<a\n",
    "    href=\"https://i.stack.imgur.com/VrdqP.png\"\n",
    "    rel=\"nofollow noreferrer\"\n",
    "    ><img\n",
    "      src=\"https://i.stack.imgur.com/VrdqP.png\"\n",
    "      alt=\"enter image description here\"\n",
    "  /></a>\n",
    "</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf10340c-bd51-4947-b452-498fa234c9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have followed the tutorial and have successfully obtained the contents of\n",
      "  the file, but where is the file being downloaded. In addition, how do I\n",
      "  specify the location of where I want to download the file? Result on Postman\n"
     ]
    }
   ],
   "source": [
    "# first round of cleaning up\n",
    "first_s = clean_html(raw_string)\n",
    "print(clean_html(raw_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bfb8721-07d5-4b25-83e0-673006a8c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/user/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93210c30-8de5-49ba-b7ec-bfa74b072e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8964ea91-5d02-48d1-a7d6-d21b075db359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_string(text, stem=\"None\"):\n",
    "\n",
    "    final_string = \"\"\n",
    "\n",
    "    # Make lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove line breaks\n",
    "    # Note: that this line can be augmented and used over\n",
    "    # to replace any characters with nothing or a space\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    useless_words = useless_words + ['hi', 'im']\n",
    "\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "\n",
    "    # Stem or Lemmatize\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    elif stem == 'Spacy':\n",
    "        text_filtered = nlp(' '.join(text_filtered))\n",
    "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "\n",
    "    final_string = ' '.join(text_stemmed)\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5b56c3b-3be3-404c-9c0e-16f2a3aae0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "followed tutorial successfully obtained contents file file downloaded addition specify location want download file result postman\n"
     ]
    }
   ],
   "source": [
    "# second round of cleaning up\n",
    "second_s = clean_string(first_s)\n",
    "print(second_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0211e87e-7c60-4851-b41f-2488f578e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column like text_clean, \n",
    "# in case punctuation is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f1fe781-36e5-415a-9948-33ae57ae2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a4a238c-e1dd-453d-925a-6930a392b457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Regular expressions library for pattern matching\n",
    "import string # Library for dealing with string operations\n",
    "import nltk\n",
    "# Natural Language Toolkit for text processing\n",
    "nltk.download('stopwords') # Download stopwords list from NLTK\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed78e0d0-a0c2-4839-875d-13cad69f8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text) :\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower ()\n",
    "    # Remove special characters and digits using regular expressions\n",
    "    text = re.sub(r'\\d+', '', text) # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove special characters except\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "feb824b9-4f84-4a96-9549-f91550afb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords (tokens):\n",
    "    stop_words = set(stopwords.words ('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffb1c9ed-ff63-459d-ac27-cdf2f238f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lemmatization(tokens):\n",
    "    lemmatizer=nltk.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f9db8a9-d706-4684-a059-df90cae35b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    filtered_tokens = remove_stopwords (tokens)\n",
    "    lemmatized_tokens = perform_lemmatization(filtered_tokens)\n",
    "    clean_text = ' '.join(lemmatized_tokens)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae72e64e-6bb1-4e72-bab0-bf9ee43ee419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text cleaning important step text analysis involve\n"
     ]
    }
   ],
   "source": [
    "text_data = \"Text cleaning is an important step in text analysis. It involve\"\n",
    "cleaned_data = clean_text(text_data)\n",
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e140af0a-f048-4a99-bfc3-be74428149ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "followed tutorial successfully obtained content file file downloaded addition specify location want download file result postman\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = clean_text(first_s)\n",
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efeb19-388b-4e64-8074-3e7dafcbb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "035a7696-76f1-42e9-8d92-289d6038beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a52145b-0b52-4fae-b8fa-eb4ef3096247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "services = ['nat_gateway','api_gateway','route53']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "495e4faa-c037-40bb-a84a-5989d53dca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "['nat_gateway', '503 error', 'connection reset'],\n",
    "['api_gateway', 'request', 'response'],\n",
    "['route53', 'routing policy', 'failover', 'latency']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "533ce4e8-07a3-451f-8cb3-62bcfdd867a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pickle files for later use\n",
    "\n",
    "# get data \n",
    "# # Make a new directory to hold the text files\n",
    "# !mkdir data\n",
    "\n",
    "for i, c in enumerate(services):\n",
    "    with open(\"data/\" + c + \".txt\", \"wb\") as file:\n",
    "        pickle.dump(text_data[i], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2119f491-10e8-4bf4-8767-173d2774cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i, c in enumerate(services):\n",
    "    with open(\"data/\" + c + \".txt\", \"rb\") as file:\n",
    "        data[c] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61376117-3697-4afc-bd95-1ca089831404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['nat_gateway', 'api_gateway', 'route53'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ab53d772-3ac4-4567-9a7c-c971fd43c95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['route53', 'routing policy']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['route53'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03010134-e7ee-4e3c-aaf4-817c9c779083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b7f1f-108b-43fe-b9cf-6b67785cbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Common data cleaning steps on all text:\n",
    "\n",
    "Make text all lower case\n",
    "Remove punctuation\n",
    "Remove numerical values\n",
    "Remove common non-sensical text (/n)\n",
    "Tokenize text\n",
    "Remove stop words\n",
    "More data cleaning steps after tokenization:\n",
    "\n",
    "Stemming / lemmatization\n",
    "Parts of speech tagging\n",
    "Create bi-grams or tri-grams\n",
    "Deal with typos\n",
    "And more...\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ef96c62-be34-47cb-bad6-67b4573b62ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nat_gateway'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7afa1253-568f-4815-b0a8-7d25a452ca00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nat_gateway', '503 error', 'connection reset']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that our dictionary \n",
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "341b279a-314a-44f8-a4b7-c677307d86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to change this to key: , value: string \n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a64847b2-38c4-40ea-970a-c5fe953c1bd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "75f7a1fe-bc9f-4672-8b73-f8b1fad7b00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>api_gateway</th>\n",
       "      <td>api_gateway request response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nat_gateway</th>\n",
       "      <td>nat_gateway 503 error connection reset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route53</th>\n",
       "      <td>route53 routing policy failover latency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "api_gateway             api_gateway request response\n",
       "nat_gateway   nat_gateway 503 error connection reset\n",
       "route53      route53 routing policy failover latency"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['description']\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49c65652-a3ba-4741-af5c-2c47eca99eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'route53 routing policy failover latency'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the transcript for Ali Wong\n",
    "data_df.description.loc['route53']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb4fd3b9-fc01-468a-864e-8c93f46a75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29a09443-f800-4ba2-8fbc-d97f025b62e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>api_gateway</th>\n",
       "      <td>apigateway request response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nat_gateway</th>\n",
       "      <td>natgateway  error connection reset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route53</th>\n",
       "      <td>routing policy failover latency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description\n",
       "api_gateway         apigateway request response\n",
       "nat_gateway  natgateway  error connection reset\n",
       "route53         routing policy failover latency"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_df.description.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "53e59e78-a761-49b6-b6d0-1ea241583544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7a89b81a-9649-4385-9c80-2edc0aafd121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>api_gateway</th>\n",
       "      <td>apigateway request response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nat_gateway</th>\n",
       "      <td>natgateway  error connection reset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route53</th>\n",
       "      <td>routing policy failover latency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description\n",
       "api_gateway         apigateway request response\n",
       "nat_gateway  natgateway  error connection reset\n",
       "route53         routing policy failover latency"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_clean.description.apply(round2))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2090fe2b-5120-4413-9d8e-e767cc8ab9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of text pre-processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "55f3bf93-d673-4a7a-b9a6-1b2e3a0fdc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>api_gateway</th>\n",
       "      <td>api_gateway request response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nat_gateway</th>\n",
       "      <td>nat_gateway 503 error connection reset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route53</th>\n",
       "      <td>route53 routing policy failover latency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "api_gateway             api_gateway request response\n",
       "nat_gateway   nat_gateway 503 error connection reset\n",
       "route53      route53 routing policy failover latency"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f777b52d-a50c-46e7-a87b-469866b93d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>api_gateway</th>\n",
       "      <td>api_gateway request response</td>\n",
       "      <td>nat_gateway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nat_gateway</th>\n",
       "      <td>nat_gateway 503 error connection reset</td>\n",
       "      <td>api_gateway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route53</th>\n",
       "      <td>route53 routing policy failover latency</td>\n",
       "      <td>route53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description      service\n",
       "api_gateway             api_gateway request response  nat_gateway\n",
       "nat_gateway   nat_gateway 503 error connection reset  api_gateway\n",
       "route53      route53 routing policy failover latency      route53"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['service'] = services\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d7e666a-fe04-4fc0-81f2-9edad2d2adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_df.to_pickle(\"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "133d05e6-6be4-42c3-b408-ec07b0db8d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDocument-Term Matrix\\nFor many of the techniques we'll be using in future notebooks, \\nthe text must be tokenized, meaning broken down into smaller pieces. \\nThe most common tokenization technique is to break down text into words. \\nWe can do this using scikit-learn's CountVectorizer, where every row will \\nrepresent a different document and every column will represent a different word.\\nIn addition, with CountVectorizer, we can remove stop words. Stop words \\nare common words that add no additional meaning to \\ntext such as 'a', 'the', etc.\\n\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Document-Term Matrix\n",
    "For many of the techniques we'll be using in future notebooks, \n",
    "the text must be tokenized, meaning broken down into smaller pieces. \n",
    "The most common tokenization technique is to break down text into words. \n",
    "We can do this using scikit-learn's CountVectorizer, where every row will \n",
    "represent a different document and every column will represent a different word.\n",
    "In addition, with CountVectorizer, we can remove stop words. Stop words \n",
    "are common words that add no additional meaning to \n",
    "text such as 'a', 'the', etc.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2abf8a16-6079-423f-b169-f3d506fe7387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apigateway</th>\n",
       "      <th>connection</th>\n",
       "      <th>error</th>\n",
       "      <th>failover</th>\n",
       "      <th>latency</th>\n",
       "      <th>natgateway</th>\n",
       "      <th>policy</th>\n",
       "      <th>request</th>\n",
       "      <th>reset</th>\n",
       "      <th>response</th>\n",
       "      <th>routing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>api_gateway</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nat_gateway</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             apigateway  connection  error  failover  latency  natgateway  \\\n",
       "api_gateway           1           0      0         0        0           0   \n",
       "nat_gateway           0           1      1         0        0           1   \n",
       "route53               0           0      0         1        1           0   \n",
       "\n",
       "             policy  request  reset  response  routing  \n",
       "api_gateway       0        1      0         1        0  \n",
       "nat_gateway       0        0      1         0        0  \n",
       "route53           1        0      0         0        1  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.description)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fc24f14c-941e-4939-916b-dbec52ce0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_dtm.to_pickle(\"dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "11155fcc-a183-4c13-8d0a-8fe53e627c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
    "data_clean.to_pickle('data_clean.pkl')\n",
    "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d63abf-cbb0-483f-96f9-f0ebdebd0f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
