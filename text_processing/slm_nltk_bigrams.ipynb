{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6ee45fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use small language model for setence generation\n",
    "# build a basic chat model \n",
    "import os\n",
    "import random \n",
    "import nltk \n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, FreqDist, ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d6f6296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/user/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "622c2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory to store the text files \n",
    "os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a6af3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the fileids in the reuters corpus \n",
    "for fileid in reuters.fileids():\n",
    "    article_text = ' '.join(reuters.words(fileid))\n",
    "    filename = f'data/{fileid.replace(\"/\", \"_\")}.txt'\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2afa8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10788   10788  186592\r\n"
     ]
    }
   ],
   "source": [
    "!ls data | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "245acd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import os \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15a71d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep training documents in a folder named 'data'\n",
    "input_data_dir = 'data'\n",
    "\n",
    "# string punction without the full stop\n",
    "# or remove full stop sign from punctuation \n",
    "punctuation = string.punctuation.replace('.', '')\n",
    "\n",
    "# check hidden files\n",
    "def is_hidden(filepath):\n",
    "    return os.path.basename(filepath).startswith('.')\n",
    "\n",
    "text_data = ''\n",
    "for filename in os.listdir(input_data_dir):\n",
    "    filepath = os.path.join(input_data_dir, filename)\n",
    "    if not is_hidden(filepath):\n",
    "        with open(filepath) as infile:\n",
    "            for line in infile:\n",
    "                if line.strip():   # skip empty line\n",
    "                    for char in punctuation:\n",
    "                        line = line.replace(char, '')\n",
    "                    text_data += line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0df919d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8441833"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combined text has 8.4 million chars\n",
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bfdb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text into words\n",
    "# lowercasing for consistency\n",
    "words = nltk.word_tokenize(text_data.lower())\n",
    "\n",
    "# generate bigrams\n",
    "bi_grams = list(bigrams(words))\n",
    "\n",
    "# calculate frequency distribution for each bigrams\n",
    "bi_gram_freq_dist = FreqDist(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f99235e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('weinberger', 'opposes'), 2)\n",
      "(('opposes', 'fujitsu'), 1)\n",
      "(('fujitsu', 'buying'), 1)\n",
      "(('buying', 'u'), 7)\n",
      "(('u', '.'), 5763)\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "# print the first five elements of the dictionary\n",
    "first_five_items = list(islice(bi_gram_freq_dist.items(), 5))\n",
    "for item in first_five_items:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3715e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute conditional frequency distribution of bigrams\n",
    "bi_gram_freq = ConditionalFreqDist(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1e8b648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'gas': 216, 'rubber': 39, 'resources': 9, 'float': 3, 'for': 3, 'lt': 2, 'disasters': 2, 'that': 2, 'source': 1, 'lower': 1, ...})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram first word is 'natural', and second word is printed\n",
    "# define the first word in bigram\n",
    "bi_gram_freq['natural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f466fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep top 3 words\n",
    "import heapq\n",
    "\n",
    "topk = 3 \n",
    "# create a dictionary to hold the top topk bigrams for each first word\n",
    "top_bigrams_per_first_word = {}\n",
    "\n",
    "# iterate over the bigram frequency distribution \n",
    "for (first_word, second_word), freq in bi_gram_freq_dist.items():\n",
    "    # initialize an empty heap for the first_word if not exist\n",
    "    if first_word not in top_bigrams_per_first_word:\n",
    "        top_bigrams_per_first_word[first_word] = [] \n",
    "        \n",
    "    # add to the heap aand maintain top topk \n",
    "    heapq.heappush(top_bigrams_per_first_word[first_word],\n",
    "                  (freq, second_word))\n",
    "    if len(top_bigrams_per_first_word[first_word]) > topk:\n",
    "        heapq.heappop(top_bigrams_per_first_word[first_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f48cb796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 'resources'), (216, 'gas'), (39, 'rubber')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bigrams_per_first_word['natural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "badaaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the heap to a simple list for each first word \n",
    "for first_word in top_bigrams_per_first_word:\n",
    "    sorted_bigrams = sorted(\n",
    "        top_bigrams_per_first_word[first_word], reverse=True\n",
    "        )\n",
    "    top_bigrams_list = []\n",
    "    for freq, second_word in sorted_bigrams:\n",
    "        top_bigrams_list.append(second_word)\n",
    "    top_bigrams_per_first_word[first_word] = top_bigrams_list\n",
    "    \n",
    "# use these filtered bigram to create a ConditionalFreqDist\n",
    "filtered_bi_grams = [] \n",
    "for first_word in top_bigrams_per_first_word:\n",
    "    for second_word in top_bigrams_per_first_word[first_word]:\n",
    "        filtered_bi_grams.append((first_word, second_word))\n",
    "                                 \n",
    "bi_gram_freq = ConditionalFreqDist(filtered_bi_grams)                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90ec7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(word, num_words):\n",
    "    word = word.lower()\n",
    "    for _ in range(num_words):\n",
    "        print(word, end=' ')\n",
    "        next_words = [item for item, freq in bi_gram_freq[word].items()]\n",
    "        if len(next_words) > 0:\n",
    "            # randomly choose a next word\n",
    "            word = random.choice(next_words)\n",
    "        else:\n",
    "            break    # break if the word has no following words\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d1215744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural gas reserves to the company said . s . 5 . s . the dollar . the dollar and other countries and other than the u . 5 . \n"
     ]
    }
   ],
   "source": [
    "# generate 30 words after \"natural\"\n",
    "generate_sentence('natural', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448a7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934424a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
